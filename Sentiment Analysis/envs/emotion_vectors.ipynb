{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open subtitles \n",
    "\n",
    "In this segement we are using opus opensubtitles 2016 parallel corpora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib2\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def getMovieName(docId):\n",
    "    user_agent = 'Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.9.0.7) Gecko/2009021910 Firefox/3.0.7'\n",
    "    url = \"http://www.opensubtitles.org/en/subtitles/{0}\".format(docId)\n",
    "\n",
    "    headers={'User-Agent':user_agent,} \n",
    "\n",
    "    req = urllib2.Request(url, headers=headers)\n",
    "\n",
    "    try:\n",
    "        page = urllib2.urlopen(req)\n",
    "        \n",
    "        content = page.read()\n",
    "        soup = BeautifulSoup(content, 'html.parser')\n",
    "\n",
    "        text = soup.find('span', {'itemprop': 'name'}).text\n",
    "            \n",
    "        return text[:text.rfind('subtitles')-1]\n",
    "    \n",
    "    except urllib2.HTTPError, e:\n",
    "        print e.fp.read()\n",
    "        return 'no name found'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 Angry Men\n"
     ]
    }
   ],
   "source": [
    "print getMovieName(3127877)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'C:\\\\Users\\\\hp\\\\Anaconda3\\\\envs\\\\minor\\\\project1\\\\Lib\\\\site-packages'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mock Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from babelnet.structures import Tree\n",
    "import numpy as np\n",
    "\n",
    "mockTree = Tree(np.array([0., 0.]), [\n",
    "        Tree(np.array([2.,2.])), \n",
    "        Tree(np.array([3.,3.]), [\n",
    "                Tree(np.array([4.,4.])),\n",
    "                Tree(np.array([4.,4.])),\n",
    "                Tree(np.array([4.,4.])), \n",
    "                Tree(np.array([5.,5.]), [\n",
    "                            Tree(np.array([1., 1.0])),\n",
    "                            Tree(np.array([1.,1.]))\n",
    "                        ])\n",
    "            ]), \n",
    "        Tree(np.array([1.,1.]))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed word representation and Semantic Trees\n",
    "\n",
    "Here we are taking advantage of semantic trees created with the help of BabelNet API and distributed word representation obtained by training skip-gram model with negative sampling on both polish and english wikipedia to automatically annotate movie subtitles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import string\n",
    "\n",
    "from display_sense_tree import load\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getSentenceVector(sentence, params):    \n",
    "    word_list = word_tokenize(sentence)\n",
    "    \n",
    "    model = params['model']\n",
    "    blacklist = params['blacklist']\n",
    "    \n",
    "    filtered_words = [word.lower() for word in word_list if word.lower() not in blacklist]                       \n",
    "    sentence_vector = sum([model[word] for word in filtered_words if word in model])\n",
    "    \n",
    "    return sentence_vector\n",
    "\n",
    "def getVector(tree, params):\n",
    "    word = tree.value.lemma\n",
    "    model = params['model'] \n",
    "    return model[word] if word in model else np.zeros(model.layer1_size)\n",
    "\n",
    "def tree2vector(tree, params, f = getVector):        \n",
    "    \n",
    "    def getvector(tree, current_weight, level):                                     \n",
    "        \n",
    "        tree_value = f(tree, params)\n",
    "        \n",
    "        node_value =  tree_value\n",
    "        node_weight = 1.0 / len(tree.children) if tree.children else 1               \n",
    "\n",
    "        if tree.children:        \n",
    "            for leaf in tree.children:      \n",
    "                node_value += node_weight*getvector(leaf, node_weight, level + 1)  \n",
    "        \n",
    "        return node_value / level\n",
    "    \n",
    "    return getvector(tree, 1, 1)\n",
    "\n",
    "def getEmotionVectors(path, params):\n",
    "    emotions = {}\n",
    "    for file in os.listdir(path):\n",
    "        if file.endswith(\".pickle\"):\n",
    "            tree = load('{0}/{1}'.format(path, file))\n",
    "            emotions[file[:file.find('.')]] = tree2vector(tree, params)\n",
    "\n",
    "    return emotions\n",
    "\n",
    "def closest(sentence_vector, emotions):\n",
    "    \n",
    "    closest = None\n",
    "    best_distane = -float('inf')\n",
    "\n",
    "    for name, emotion_vector in emotions.iteritems():        \n",
    "        current_distance = cosine_similarity([sentence_vector], [emotion_vector])[0][0]    \n",
    "        if best_distane < current_distance:\n",
    "            best_distane = current_distance\n",
    "            closest = name\n",
    "    \n",
    "    return closest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0.]\n",
      "\t[2. 2.]\n",
      "\t[3. 3.]\n",
      "\t\t[4. 4.]\n",
      "\t\t[4. 4.]\n",
      "\t\t[4. 4.]\n",
      "\t\t[5. 5.]\n",
      "\t\t\t[1. 1.]\n",
      "\t\t\t[1. 1.]\n",
      "\t[1. 1.]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print mockTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.23958333, 1.23958333])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree2vector(mockTree, {}, lambda tree, params: np.array(tree.value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\hp/nltk_data'\n    - 'C:\\\\Users\\\\hp\\\\Anaconda3\\\\envs\\\\minor\\\\nltk_data'\n    - 'C:\\\\Users\\\\hp\\\\Anaconda3\\\\envs\\\\minor\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\hp\\\\Anaconda3\\\\envs\\\\minor\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\hp\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-b0977f83a46e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m#pl_model = Word2Vec.load('/home/models/wiki.pl.model')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0men_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'model'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0men_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'blacklist'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'english'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpunctuation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;31m#pl_params = {'model': pl_model, 'blacklist': set(stopwords.words('polish')).union(set(string.punctuation))}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\hp\\Anaconda3\\envs\\minor\\project1\\Lib\\site-packages\\nltk\\corpus\\util.pyc\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    121\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 123\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    124\u001b[0m         \u001b[1;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m         \u001b[1;31m# __class__ to something new:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\hp\\Anaconda3\\envs\\minor\\project1\\Lib\\site-packages\\nltk\\corpus\\util.pyc\u001b[0m in \u001b[0;36m__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     86\u001b[0m                     \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'{}/{}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m                     \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m         \u001b[1;31m# Load the corpus.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\hp/nltk_data'\n    - 'C:\\\\Users\\\\hp\\\\Anaconda3\\\\envs\\\\minor\\\\nltk_data'\n    - 'C:\\\\Users\\\\hp\\\\Anaconda3\\\\envs\\\\minor\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\hp\\\\Anaconda3\\\\envs\\\\minor\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\hp\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "MODEL_PATH = 'C:/Users/hp/Anaconda3/envs/minor/project1/Lib/site-packages/'\n",
    "import gensim \n",
    "en_model = gensim.models.KeyedVectors.load_word2vec_format(MODEL_PATH + 'GoogleNews-vectors-negative300.bin', binary=True)\n",
    "\n",
    "#en_model = Word2Vec.load('/home/models/wiki.en.model')\n",
    "#pl_model = Word2Vec.load('/home/models/wiki.pl.model')\n",
    "\n",
    "en_params = {'model': en_model, 'blacklist': set(stopwords.words('english')).union(set(string.punctuation))}\n",
    "#pl_params = {'model': pl_model, 'blacklist': set(stopwords.words('polish')).union(set(string.punctuation))}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### English test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'en_params' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-46e276026150>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mbuild_emotion_vectors\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mgetSentenceVector\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgetEmotionVectors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclosest\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0msentence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetSentenceVector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"To continue , you 've listened to a long and complex case , murder in the first degree .\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0men_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0men_emotions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetEmotionVectors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/home/models/babelnet/en/'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0men_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'en_params' is not defined"
     ]
    }
   ],
   "source": [
    "#import build_emotion_vectors\n",
    "from build_emotion_vectors import getSentenceVector, getEmotionVectors, closest\n",
    "\n",
    "sentence = getSentenceVector(\"To continue , you 've listened to a long and complex case , murder in the first degree .\", en_params)\n",
    "en_emotions = getEmotionVectors('/home/models/babelnet/en/', en_params)\n",
    "\n",
    "closest(sentence, en_emotions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polish test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentence = getSentenceVector('Kontynuując , wysłuchali panowie długiego i złożonego procesu dotyczącego morderstwa pierwszego stopnia .', pl_params)\n",
    "pl_emotions = getEmotionVectors('/home/models/babelnet/pl/', pl_params)\n",
    "\n",
    "closest(sentence, pl_emotions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open subtitles 2016\n",
    "\n",
    "Now we will test wheter there is a correlation between emotions in emotions discovered in polish and english subtitles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from babelnet import opensubtitles as o\n",
    "\n",
    "# to reduce the amount of stuff downloaded from the internet we would like to cache as much as possible\n",
    "class Memoize:\n",
    "    def __init__(self, f):\n",
    "        self.f = f\n",
    "        self.memo = {}\n",
    "    def __call__(self, *args):\n",
    "        if not args in self.memo:\n",
    "            self.memo[args] = self.f(*args)\n",
    "        return self.memo[args]\n",
    "\n",
    "o.getParalelSubtittles = Memoize(o.getParalelSubtittles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting\n",
    "\n",
    "To plot movie plot (pun intended) we would like to nicely accumulate emotions from given \"subtitle stream\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "class DictCounter(object):\n",
    "    def __init__(self, bufferSize, keys):\n",
    "        self.bufferSize = bufferSize\n",
    "        self.buffer = []\n",
    "        self.results = []\n",
    "        self.keys = keys\n",
    "    \n",
    "    def __flush__(self):\n",
    "        self.results.append(Counter(self.buffer))\n",
    "        self.buffer = []\n",
    "    \n",
    "    def add(self, item):\n",
    "        self.buffer.append(item)\n",
    "        if len(self.buffer) >= self.bufferSize:\n",
    "            self.__flush__()\n",
    "            \n",
    "    def collect(self, normalize = True):\n",
    "        if self.buffer:\n",
    "            self.__flush__()\n",
    "        \n",
    "        data = {key:[] for key in self.keys}\n",
    "        \n",
    "        for counter in self.results:                       \n",
    "            normalizing_factor = 1.0 / sum(counter.values()) if normalize else 1\n",
    "            for key in self.keys:\n",
    "                data[key].append(round(normalizing_factor * counter[key], 2))\n",
    "        \n",
    "        return data       \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot\n",
    "\n",
    "This is acctual code for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plotMovie(data_1, data_2, title = 'Movie', color_map = 'plasma', titlesize = 18):\n",
    "\n",
    "    cmap =  plt.cm.get_cmap(color_map)\n",
    "    \n",
    "    f, (ax1, ax2) = plt.subplots(2, 1, figsize=(20, 7))\n",
    "\n",
    "    # positions of the left bar-boundaries\n",
    "    keys_size = float(len(data_1.keys()))\n",
    "    data_size = len(data_1[data_1.keys()[0]])\n",
    "    bar_l = [i+1 for i in range(data_size)] \n",
    "\n",
    "    # positions of the x-axis ticks (center of the bars as bar labels)\n",
    "    tick_pos = [i+(1/2) for i in bar_l] \n",
    "    \n",
    "    def drawplot(data, suffix, ax):\n",
    "        cum = None\n",
    "        for (index, (label, series)) in enumerate(data.items()):\n",
    "\n",
    "            # plot bar\n",
    "            ax.bar(\n",
    "                bar_l, series,\n",
    "                bottom=cum, label=label,\n",
    "                alpha=0.8, color = cmap(index / keys_size),\n",
    "                width=1.0, linewidth = 0.0\n",
    "            )    \n",
    "\n",
    "            cum = series if not cum else [x + y for x, y in zip(cum, series)]\n",
    "\n",
    "        plt.sca(ax)\n",
    "        ax.set_ylabel(\"Frequency\")\n",
    "        ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "        ax.set_title('{0} {1}'.format(title, suffix), fontsize = titlesize)\n",
    "        plt.xlim([min(tick_pos), max(tick_pos)])\n",
    "        plt.ylim(0, 1)\n",
    "        plt.grid()\n",
    "\n",
    "    ###################\n",
    "    ## First Data\n",
    "    ###################    \n",
    "    drawplot(data_1, \"EN\", ax1)\n",
    "    \n",
    "    ###################\n",
    "    ## Second Data\n",
    "    ###################\n",
    "    drawplot(data_2, \"PL\", ax2)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def colsestEmotion(text, emotion_vectors, params):\n",
    "    sentence_vector = getSentenceVector(text, params)\n",
    "    return closest(sentence_vector, emotion_vectors)\n",
    "\n",
    "def extractEmotions(doc, buffLimit = 10, limit = None, normalize = True):\n",
    "                    \n",
    "    en_acc = DictCounter(buffLimit, en_emotions.keys())\n",
    "    pl_acc = DictCounter(buffLimit, pl_emotions.keys())                    \n",
    "                    \n",
    "    for en_text, pl_text in itertools.islice(o.getParalelSubtittles(doc), 0, limit, 1):\n",
    "        \n",
    "        # Try not to die\n",
    "        try:\n",
    "            em_en = colsestEmotion(en_text, en_emotions, en_params)\n",
    "            en_acc.add(em_en)\n",
    "        except:\n",
    "            print 'Fallback to emotionless [en]', en_text\n",
    "            en_acc.add('emotionless')\n",
    "        \n",
    "        # Try not to die\n",
    "        try:\n",
    "            em_pl = colsestEmotion(pl_text, pl_emotions, pl_params)\n",
    "            pl_acc.add(em_pl)\n",
    "        except:\n",
    "            print 'Fallback to emotionless [pl]', pl_text\n",
    "            pl_acc.add('emotionless')                                                        \n",
    "\n",
    "    return en_acc.collect(normalize), pl_acc.collect(normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 12 Angry Men"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc = \"en/1957/50083/3127877.xml.gz\"\n",
    "\n",
    "data_en, data_pl = extractEmotions(doc, buffLimit = 10)\n",
    "\n",
    "plotMovie(data_en, data_pl, '12 Angry Men', titlesize = 18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Casablanca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc = \"en/1942/34583/3098261.xml.gz\"\n",
    "\n",
    "data_en, data_pl = extractEmotions(doc, buffLimit = 10)\n",
    "plotMovie(data_en, data_pl, 'Casablanca', titlesize = 18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pulp Fiction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc = \"en/1994/110912/3133987.xml.gz\"\n",
    "\n",
    "data_en, data_pl = extractEmotions(doc, buffLimit = 10)\n",
    "plotMovie(data_en, data_pl, 'Pulp Fiction', titlesize = 18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_en, data_pl = extractEmotions(\"en/1994/110912/3133987.xml.gz\", buffLimit = 10, normalize= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sum([sum(x) for x in data_en.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for k,v in data_en.iteritems():\n",
    "    print k, sum(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sum([sum(x) for x in data_pl.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for k,v in data_pl.iteritems():\n",
    "    print k, sum(v)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project1",
   "language": "python",
   "name": "project1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
